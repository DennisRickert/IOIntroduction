---
title: "Session 3: Introduction into R and Econometrics"
author: "Dennis Rickert, MINES Paris-PSL University"
date: "2023-07-17"
output: 
  ioslides_presentation: 
#  html_document: 
#    number_sections: yes
#    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction 

##
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("College_NewYorker.jpg")
```

## Overview of Session 3+4

What we do in Session 3+4

- Intro: Intuitive examples of causality vs correlation
- Understanding the causal effect of price on demand
- 


## Learning objectives

- 

## Concept of Simple Regressions

Assume two variables $x$ and $y$ from a population (e.g., clients) 

- "Explaining $y$ in terms of $x$," or 
- "studying how $y$ changes when we change $x$."


Some examples include:

- $y$ is the *demand* of ice cream and $p$ is the price
- $y$ is  *market value* of a footballer and $x$ the *goals* he scored
- $y$ is the *quality of wine* and $x$ the *weather*
- $y$ is  *soybean crop yield* and $x$ is amount of fertilizer;
- $y$ is *hourly wage* and $x$ is *years of education*;

## The Simple Regression Model

We confront three issues, if we want to “explain $y$ in terms of $x$”

- Which functional relationship do we assume (e.g., linear)? 
- How do we allow factors, other than $x$, to affect $y$? 
- How can we ensure a causal relationship between $y$ and $x$?  
  - *ceteris paribus* notion: holding constant other factors

## To address those 3 issues, we define

$$y=\beta_0+\beta_1x+u$$

- Linear relation between observables $x$ and $y$ (*Issue 1*)
- $\beta_1:$ **slope parameter** in the relation between $y$ and $x$
- $\beta_0:$ **intercept parameter** aka the **constant term**
- Error term $u:$ Unobservables allowed to enter (*Issue 2*)
- Certain assumptions to get causal effect of $x$ on $y$ (*Issue 3*)  

Main intuition: given we observe $x$ and $y$

- $\beta_1$ and $\beta_0$: estimated making assumptions on $u$
- $\beta_1$: how changes in $x$ affect changes in $y$ given $\beta_0$ and $u$

## We get the causal effect of $x$ on $y$ 

holding all else equal (using ceteris paribus notion) 

- i.e., the the unobserved factors in $u$ are held fixed, 
- so that the change in $u$ is zero, i.e., $\Delta{u}$= 0, 
- i.e., $u$ does not affect $y$
$$\Delta{y}=\beta_1\Delta{x}~~~~if~~~~\Delta{u}=0$$ 

- then $x$ has a linear effect on $y$ 
- where $\beta_1$ yields causal effect of $x$ on $y$ 


## Example: Soybean Yield and Fertilizer
  
Suppose that soybean yield per land unit is determined by:  
      
$$yield=\beta_0+\beta_1 fertilizer + u$$

where $y= yield$ and $x=fertilizer$. 

- Coefficient $\beta_1$ measures the effect of fertilizer on yield, holding unobservable factors fixed  
- Error term $u$ contains factors, such as, land quality, rainfall, ... 
- $\beta_0$ captures factors that are constant across land units

$$\Delta{yield}=\beta_1\Delta fertilizer$$

## Example: A Simple Wage Equation
  
Relating a person's wage to education and unobservables:  
          
$$wage=\beta_0+\beta_1 educ + u$$ 

- $wage$ is dollars earned per hour and $educ$ is education years, 
- $\beta_1$ measures the change in hourly wage given another year of education, holding all other factors fixed. 
- Some of the unobserved factors include 
  - labor force experience, 
  - innate ability, 
  - tenure with current employer, 
  - work ethic, 

$$\Delta{wage}=\beta_1\Delta educ$$



## Example: Bob's Ice Cream Business

Similarly, $\beta_1$ measures the causal effect of price on quantity

$$quantity=\beta_0+\beta_1 price + u$$

if we hold unobservable factors in $u$ fixed, 

- e.g., seasonality has no systematic impact on quantity

Only then we get the causal effect: 

$$\Delta{quantity}=\beta_1\Delta price$$

## Zero conditional mean assumption

How can we learn about the causal effect of $x$ on $y$, holding other factors fixed, if we ignore all those other factors?

We assume: $\mathbb{E}[u|x]=\mathbb{E}[u]=0$

Take the expected value of $y|x$.   

Then: $\mathbb{E}[y|x]= \mathbb{E}[\beta_0 +\beta_1x + u|x] = \beta_0 +\beta_1x + \mathbb{E}[u|x]$

which yields

$$\mathbb{E}(y|x)=\beta_0+\beta_1x$$

$\beta_1$ is causal due to this *zero conditional mean assumption*

## Harmless assumption $E(u)=0$

.. says nothing about the relation between $u$ and $x$, but simply makes a statement about the distribution of unobservables

We can always redefine the intercept in $y=\beta_0+\beta_1x+u$ to make $\mathbb{E}(u)=0$ true.

  - Soybean example: we can normalize the unobserved factors affecting soybean yield, such as land quality, to 0
  - Wage example: We can assume that things such as average ability are zero in the population of all working people.


## Less harmless: mean independence

or $\mathbb{E}(u|x)=u$ 

- for any $x$, we can obtain the expected (or average) value of $u$ for that slice of the population described by the value of $x$.
- The crucial assumption is that the average value of $u$ does not depend on the value of $x$.
- When assumption $E(u|x)=E(u)$ holds, we say that $u$ is **mean independent** of $x$. 
- When we combine mean independence with assumption $\mathbb{E}(u)=0$, we obtain the **zero conditional mean assumption**, $E(u|x)=0$.

## Wage example with $u=abil$

Equation $\mathbb{E}(u|x)=\mathbb{E}(u)$ requires that the average level of ability is the same, regardless of years of education $x$.
  
- For example, if $E(abil|8)$ denotes the average ability for the group of all people with eight years of education, 
- and $E(abil|16)$ denotes the average ability among people in the population with sixteen years of education, 
- $E(u|x)=E(u)$ implies that $E(abil|8)=E(abil|18)$

Thus, the average ability the same for all education levels.

## Is this a reasonable assumption?


If, for example, we think that average ability increases with years of education, then equation $E(u|x)=E(u)$ is false.

- This would happen if, on average, people with more ability choose to become more educated.
- As we cannot observe innate ability, we have no way of knowing whether or not average ability is the same for all education levels.
- But: this is an issue that we must address before relying on simple regression analysis!

## Bob's ice cream business 

For equation $\mathbb{E}(u|x)=\mathbb{E}(u)$ with

- $x=price$
- $u=seasonality$

it must hold $\mathbb{E}(summer|price)=\mathbb{E}(winter|price)$

- i.e., $u=seasonality$ is uncorrelated to $x=prices$ 
- or the level of demand does not depend on price
- But we do know that demand is higher in summer, which is why we set higher prices  




## The Ordinary Least Squares Estimator

How to estimate the parameters $\beta_0$ and $\beta_1$ in equation $y=\beta_0+\beta_1x+u$?

- To do this, we need a sample from the population.
- Let {($x_i,y_i):i=1,...,n$} denote a random sample of size $n$ from the
population.
- We can write $y_i=\beta_0+\beta_1x_i+u_i$ for each $i$.

As an example, $x_i$ might be the annual income and $y_i$ the annual savings for family $i$ during a particular year.

- If we have collected data on 15 families, then _n=15_.


## Savings and income for 15 families

aim is to get: $\mathbb{E}(savings|income)=\beta_0+\beta_1 income$


```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_2.png")
```

##  OLS Graph: Fitted values and residuals

Idea: choose $(\beta_0,\beta_1)$ to minimizes the squared sum of $u_i$

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_4.png")
```



## The Ordinary Least Squares Estimator

The OLS estimator has the following objective function:

$$(\hat{\beta}_{0}^{OLS}, \hat{\beta}_{1}^{OLS})=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i\hat{u}_i^2]}=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i(\hat{y}_i-y_i)^2]}$$ 

Algorithm chooses $(\hat{\beta}_{0}, \hat{\beta}_{1})$ until $\hat{u}_i^2$ reaches global minimum

- $\hat{y}_i$ are the predicted values, 
- $\hat{\beta_0}$ and $\hat{\beta_1}$ are estimated coefficients, 
- $\hat{u}_i$ are residuals


## OLS: how to estimate $\beta_0$ and $\beta_1$?

The ordinary least squares (OLS) estimators are  

$$\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{Cov(x,y)}{Var(x)}$$

$$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$

Based on these estimated parameters, the OLS regression line is  

$$\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$$

Given observations $x_i$ and $y_i$, we just need to calculate 

- the mean of $\bar{y},\bar{x}$
- more commonly $Cov(x,y)$ and $Var(x)$. 



## Economist Ashenfelter vs. wine experts

There are differences in price and quality of wine from year to year that are sometimes very significant: 

- wines are tasting better when they are older. 
- So, there is an incentive to store young wines until they are mature. 
- Wine experts taste the wine and then predict which ones will be the best later. 
- But it is hard to determine the quality of the wine when it is so young just by tasting it, 
  - since the taste will change significantly by the time it will be consumed.
- Ashenfelter used Econometrics instead of believing experts

##
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_nyt.jpg")
```

- Calculate the winter rain and the harvest rain (in millimeters). 
- Add summer heat in the vineyard (in degrees centigrade). 
- What do you have? 
- A very, very passionate argument over wine.


##
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_forbes.jpg")
```


## 
<iframe width="560" height="315" src="https://www.youtube.com/embed/8WMRj9mTQtI" frameborder="0" allowfullscreen></iframe>

## Have a look at the paper and answer the following questions
1. Which of the following predicts the prices of older, mature wines?',['Dry Summers','Humid Summers','Cold Winters','Cold Summers'],'Dry Summers')
2. What is true about the relationship between Bordeaux wine and rain in August',['Positively correlated + statistically insignificant','Positively correlated + statistically significant','Negatively correlated + statistically insignificant','Negatively correlated + statistically significant'],'Negatively correlated + statistically significant')
3. Which independent variable has the largest (statistically significant) effect on the price of Bordeaux wine?',['Age of the vintage','Average temperature in growing season','Rain in August','Average temperature in September'],'Average temperature in growing season
4. With every additional year, the price of a vintage increases by (column 2)',['0.024%','0.048%','2.4%','4.8%'],'2.4%'
5. According to the table above, the famous Ashenfelter Wine Equation can be represented as:                                      0.024 Age + 0.62 Temp. (growing season) – 0.004 August rain + 0.001 Pre-vintage rain + 0.008 Temp. (September)',['Yes','No'],'No')
6. It is a good example for regressions because wine prices have an effect on weather but not weather on wine prices',['Yes','No','Maybe'],'No'
7. Concluding from the question above, we do not have to care about unobserved confounding factors',['Yes','No','Maybe'],'No'


##  Have a look at this table from the research paper. 

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_table2.jpg")
```


## Let's have a look how the magic wine equation works

1. Load the data set ('wine.csv').
2. Calculate the average price of wine. Explain why this is not a good predictor for future prices. 
3. Explain under which circumstances OLS is the preferred choice and why.  
4. Regress average temperature in the growing season on prices. What do you see? How do you interpret the slope?
5. How do you measure the "fit" of a model? Which of the two models has the better fit? 
6. And are there even better models?
7. Show if OLS assumptions are violated

## 
df2 = pd.read_csv("C:/Users/Anonymouse/Dropbox (CERNA)/Econometrics MINES/wine.csv")

## A simple linear regression model satisfies the following relationship for all observations $t=1,...,T$

$$y_{t}=\beta_{0}+\beta_{1}x_{t}+\varepsilon_{t}$$

  - $y=(y_{1},...y_{T})$ is the dependent variable.
  - $x=(x_{1},...x_{T})$ is the explanatory variable.
  - $\varepsilon=(\varepsilon_{1},...,\varepsilon_{T})$ (epsilon) is a random variable that describes unobserved influences on $y$, sometimes called *disturbance*. We will typically make some assumptions on the distribution of $\varepsilon$. We will also use the letters $u$ and $\eta$ (eta) to denote disturbances.
  - $\beta=(\beta_{0},\beta_{1})$ is the vector of true coefficients.

## [2col] Estimate, Predicted Value and Residuum

##. leftcol

- Let $\hat{\beta}$ be an *estimate* of the true parameter vector $\beta$.

- The *predicted values* (also called fitted values) of $y$ are given by $$\hat{y}=\hat \beta_0 + \hat \beta_1 x$$

- The *residuals* (estimated values of the disturbance) are given by $$\hat{\varepsilon}=y-\hat{y}=y-\hat \beta_0 - \hat \beta_1 x$$

  - The residuals $\hat{\varepsilon}$ are close to the true disturbances $\varepsilon$ if our estimate $\hat \beta$ is close to the true parameters $\beta$.


##. rightcol
```{r echo=FALSE, fig.width=3.2, fig.height=3, show_code="no", dev="svg"}
library(ggplot2)
# Preisexperiment
beta0 = 100
beta1 = 12
T = 20
set.seed(123456)
p = sample(seq(2,6, length=T+1), T, replace = FALSE)
eps = rnorm(T,0, 20)
D = round(beta0 - beta1*p + eps)
d = data.frame(t=seq_along(p),D=D,p=p,eps=eps)

d$y_hat = fitted(lm(data=d, D~p))
dc = d[9,]

ggplot(data=d,aes(x=p,y=D, label=t)) +
  geom_point(alpha=0.5) +
  geom_smooth(method=lm, color="red", se=FALSE) +
  geom_segment(data=dc, aes(x=p,xend=p,y=y_hat,yend=D),col="black") +
  geom_point(data=dc,color="black", size=2) +
  geom_point(data=dc,aes(y=y_hat),color="red", size=2) +
  annotate('text', x=dc$p, y=dc$D+7, color="black", size=5,  label="y[i]", parse=TRUE)+  
#  annotate('text', x=4.1, y=68, color="black", size=5,  label="hat(eps)[i]", parse=TRUE)+  
  annotate('text', x=dc$p, y=dc$y_hat-7, color="black", size=5,  label="hat(y)[i]", parse=TRUE)+  
  xlab("x") +
  ylab("y") +
  theme_bw()


```

## Ordinary Least Squares Estimation (OLS)

- An ordinary least squares (OLS) estimate minimizes the sum of squared residuals $$\hat{\beta}=\arg\min\sum_{t=1}^{T}\hat{\varepsilon}_{t}^{2}$$

- For the simple linear regression (one explanatory variable), the OLS estimator $\hat \beta_1$ has the following formula
  $$\begin{aligned}
    \hat \beta_1 &= \frac{Cov(x_t,y_t)}{Var(x_t)} =
    cor(x_t,y_t) \frac{sd(y)}{sd(x)}
  \end{aligned}$$
  where $cor$ denotes an empirical correlation and $sd$ an empirical standard deviation for our sample data. 

## Linear Regression Model in Matrix Notation

- One often writes a linear regression model in matrix notation:
  $$y=X\beta+\varepsilon$$ 
  with
  $$X=\left(\begin{array}{cccc}
    1 & x_{1}\\
    ... & ...\\
    1 & x_{T}
    \end{array}\right)=\left(\begin{array}{cccc}
    \boldsymbol{1} & x
  \end{array}\right)$$

- The OLS estimator $\hat \beta = (\hat \beta_0, \hat \beta_1)$ is then given by
$$\hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y$$
- You don't have to understand the matrix notation for this lecture, but for those who do, we will sometimes mention it.

## Estimators and estimates

- Since $y=X\beta+\varepsilon$, the OLS estimator can be rewritten as

  $$\begin{aligned}
  \hat{\beta} & =(X^{\prime}X)^{-1}X^{\prime}y\\
   & =(X^{\prime}X)^{-1}X^{\prime}(X\beta+\varepsilon)\\
   & =\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\end{aligned}$$

  - This means $\hat \beta$ is a linear transformation of the true parameters $\beta$ and the disturbance $\varepsilon$

- As a function of a random variable $\varepsilon$ the OLS *estimator* $\hat{\beta}$ is itself a random variable

- The OLS *estimate* $\hat{\beta}$ is a realization of the OLS estimator, i.e. the value for particular draws of $\varepsilon$ and $X$.

- To understand what econometrics and most of statistics is doing, one should keep in mind that *an estimator is a random variable*.

## A Monte-Carlo Simulation of \(\hat \beta\) in R

*Analysis in R* (R analysises would be shown life in the lecture and I made videos for some. You will do similar analysis yourself in an RTutor problem set)

- Start with the simulation of our ice cream model from Chapter 1a with random prices and estimate the demand function.

- Write an R function with name `sim.and.est` that performs this simulation and returns the estimated coefficients $\hat \beta$ of the demand function. Your function shall have the sample size $T$ as an argument.

- Repeat the simulation and estimation several times (draw new $\varepsilon$ each time) and compute and store the OLS estimates. Use the function `simulation.study` in the package `sktools` to conduct a systematic simulation study. Plot the distribution of the resulting estimates $\hat{\beta}_{1}$ and compare it with the true value of $\beta_{1}$. How does the distribution change if we change the sample size $T$?

#. frame Distribution of our estimator \(\hat \beta_1\)

We found in our Monte-Carlo simulation study that the estimator $\hat \beta_1$ has a distribution, that depends on the sample size $T$. Here it is shown for $T=20$ and a true $\beta_1=-1$

![image](figures/sim1b.svg)

## Standard Error of OLS estimator

- In a simple linear regression (one explanatory variable) $$y= \beta_0 + \beta_1 x + \varepsilon$$ where the $\varepsilon$ are independently, identically normal distributed, the standard deviation of the OLS estimator $\hat \beta_1$ can be *estimated* by

$$se(\hat \beta_1) = \hat{sd}(\hat \beta_1) = \frac{1}{\sqrt T} \frac{sd(\hat \varepsilon)}{sd(x)}$$
- We call this estimate of the standard deviation the *standard error* of $\hat \beta_1$.

- Observations: We can estimate $\beta_1$ more precisely if we have...

  - a larger the sample size $T$
  
  - more variation in $x$ (higher standard deviation).

*Analysis in R:* We run a linear regression with `lm` and call `summary` on the result to see besides the estimated coefficients also the standard errors.

## Robust Standard Errors

- There is also a matrix formula to compute the standard errors for all $\hat \beta$ that can also be used for multiple linear regressions with more than one explanatory variable.

- If the $\varepsilon$ are not identically, independently normal distributed, one should use approbriate *robust* standard errors. Most empirical papers in economics use some robust standard errors.

- We don't explain robust standard errors further in this course. Just note that in R a convenient way to use robust standard errors is the function `lm_robust` in the package `estimatr` or the function `felm` in the package `lfe`.

## Criteria for estimators: Bias

- **Bias:** Recall that an estimator $\hat{\beta}$ is a random variable since it depends on the realizations of $\varepsilon.$ Let $E\hat{\beta}$ be the expected value of $\hat{\beta}$. The bias of $\hat{\beta}$ measures a systematic over- or underestimation of $\hat{\beta}$ compared to $\beta$: $$Bias(\widehat{\beta})=E\hat{\beta}-\beta.$$

- **Unbiasedness:** An estimator $\hat{\beta}$ is unbiased if its Bias is 0, i.e. $$E\hat{\beta}=\beta$$

## Criteria for estimators: Standard Deviation

- For two unbiased estimators of $\beta_i$, one would typically prefer an estimator with a lower standard deviation $sd(\hat{\beta}_{i})$ (or equivalently the one with the lower variance $Var(\hat{\beta}_{i})$)

## Criteria for estimators: Mean Squared Error

- **Mean squared error**: The mean squared error of $\hat{\beta}_{i}$ is given by

    $$\begin{aligned}
    MSE(\hat{\beta}_{i}) & =E(\hat{\beta}_{i}-\beta_{i})^{2}\\
     & =Bias(\hat{\beta}_{i})^{2}+Var(\hat{\beta}_{i})\end{aligned}$$


- *Analysis in R:* We go back to the previous simulation study in R and compute an approximation to the bias and mean squared error of $\hat{\beta}_{0}$ by analysing the simulated distribution of $\hat{\beta}_{0}$. How does the MSE change when you increase the number of observations $T$ in the simulated data set?

## Criteria for estimators: Consistency

- An estimator $\hat{\beta}$ is (strongly) **consistent** if its MSE converges to 0 as the sample size $T$ grows large $$\lim_{T\rightarrow\infty}MSE(\hat{\beta})=0.$$

- A note for the mathematic students: Strong consistency implies weak consistency, which means that the estimated parameters $\widehat{\beta}$ converges (in probability) to the true parameters $\beta$ $$\underset{T\rightarrow\infty}{\mbox{plim}}\widehat{\beta}=\beta$$

- **Consistency is often seen the most important requirement for an estimator.**

- If an estimator is inconsistent that is typically because it is biased and the bias does not go away as $T\rightarrow\infty$.

## Criteria for estimators: Efficiency

- An estimator $\hat{\beta}$ is **efficient** (within a specified class of estimators) if there is no other estimator that has a lower mean squared error.

- We won't discuss efficiency deeper in this course.

## Assumptions of the simple linear regression model

- We now state a series of assumptions for the simple linear regression model (one explanatory variable).

  - A1: $E(\varepsilon_t|x)=0$

  - A2: The $\varepsilon_{t}$ are identically and independently distributed.

  - A3: The $\varepsilon_{t}$ are normally distributed

  - A1-A3 are often compactly written as $\varepsilon_{t}\overset{i.i.d.}{\sim}$ $N(0,\sigma^{2}).$

  - A4: The explanatory variable $x$ must have positive variance and be deterministic or a stationary random variable. (We don't discuss what stationary means in this course, but you can look it up on Wikipedia.)

- If all assumptions are satisfied, the OLS estimator $\hat{\beta}$ will be consistent, unbiassed and efficient.

- Good intuitive overview: “An Introduction to Econometrics” by Peter Kennedy

## Assumption A1

- **A1** No matter which values of $x$ we observe, the conditional expected value of $\varepsilon_{t}$ is always zero: $$E(\varepsilon_{t}|x)=0$$

- The important thing is not the 0 on the right. If it were a positive or negative value, we could always redefine the constant $\beta_0$ to make it 0.

- The important thing is that the expected value of $\varepsilon_t$ does not depend on $x$. This means knowing $x$ shall give us no information about the expected value of $\varepsilon_t$.
  
- In our ice cream example with profit maximizing prices this condition is violated. Higher demand shocks lead to higher prices. This means if we observe a high price, we expect that there was a positive demand shock $\varepsilon_t$.   

## Exogenous and Endogenous Variables

- We say the explanatory variable $x$ is **exogenous**, if it is uncorrelated with $\varepsilon$ $$\mbox{cor}(x_t,\varepsilon_t)=0$$

- We say $x$ is **endogenous** if $\mbox{cor}(x_t,\varepsilon_t)\ne0$

- Condition A1 $E(\varepsilon|x)=0$ can only be satisfied if $x$ is exogenous.

- We will typically just check whether $x$ is exogenous, even though A1 is a stronger condition. A1 is sometimes called *strong exogeniety*.  In all examples studied in this course, exogeniety of $x$ implies that also A1 holds.

## The problem of endogeniety

- **A1 is the most important assumption:**

  - **If $x$ is endogenous, the OLS estimator $\hat{\beta}$ will (typically) be inconsistent and biased.**

- We typically check whether there is endogeniety as follows:

  - Think of a true model for the data generating process; specify which factors are part of the random shock $\varepsilon$

  - See if in that model the explanatory variable $x$ is a function of $\varepsilon$ or of some unobserved variable that is part of $\varepsilon$. If that is the case $x$ is endogenous.

## Exploring Endogeniety in R

*Analysis in R*: We consider the ice cream model from slides 1a. 

- We compare the two cases that prices are randomly drawn and prices are chosen optimally. In which model is A1 satisfied / violated and the OLS estimator consistent / inconsistent? We compute the correlation between $\varepsilon$ and $p$ in your simulated data.

- For real world data, we never observe $\varepsilon$, i.e. we cannot simply see in the data that there is an endogeniety problem. If we compute the correlation between $p$ and $\hat{\varepsilon}$ (the OLS residual), we see that it is always 0 (or due to rounding errors maybe slightly away) even if $\varepsilon$ and $p$ are correlated.

- We now assume prices $p_{t}$ are simply always set 10% above the cost $c_{t}$ (and costs shall be uncorrelated with $\varepsilon$). Are prices $p_{t}$ then endogenous or exogenous? Is the OLS estimator consistent?

- We now consider the model in which prices are chosen optimal. Consider in your simulation the limit case that $\sigma_{\varepsilon}\rightarrow 0$. What is then the main source of variation in prices? To which value will $cor(\varepsilon,p)$ converge? We study in the simulation whether the OLS estimator of prices seems to be consistent and unbiased in this limit case.

## A2, No auto-correlation and no heteroskedasticity

- **A2** The $\varepsilon_{t}$ are identically and independently distributed.

- Typical violations of A2:

  - auto-correlation: demand shocks may be persistent across periods

  - heteroskedasticity: the variance of $\varepsilon_{t}$ can depend on the explanatory variable (this alone does not yet mean that A1 is violated)

- A2 is moderately important. If violated, the OLS estimator $\hat{\beta}$ is still consistent but not efficient. One must calculate standard errors using an appropriate formula for robust standard errors.

- We don't study violations of A2 in this course.

## A3: Normally distributed disturbances

- **A3** $\varepsilon_{t}$ is normally distributed

- It is nice if A3 holds, but it is not crucial. Even if A3 is violated, the OLS estimate $\hat{\beta}$ is the best unbiased linear estimators of $\beta$ (Gauss-Markov Theorem). Significance tests would only be asymptotically correct. If A1-A3 (and the other assumptions) holds, $\hat{\beta}$ coincides with Maximum Likelihood estimator and is efficient.

## frame 95% Confidence Intervals

- If assumptions A1 holds (no endogeniety problem) then with approximately 95% probability we find an estimate $\hat{\beta}_{i}$ such that the interval of plus-minus 2 standard errors around $\hat{\beta}_{i}$ contains the true parameter $\beta_{i}$. We call this interval $$[\hat{\beta}_{i}-2\cdot se(\hat{\beta}_{i})\;;\;\hat{\beta}_{i}+2\cdot se(\hat{\beta}_{i})]$$ the approximate **95% confidence interval**.

*Analysis in R* 

1. Run a linear regression on some simulated data without endogenitey problem and apply the R function `summary` on the result to see the estimated standard errors. Compute in your head the approximate 95% confidence interval and also use the function `confint` for the exact confidence interval.

2. Now simulate data with an endogeniety problem. Does the true parameter $\beta_{1}$ still typically lie within the 95% confidence interval around $\hat{\beta}_{1}$ or can it be that it is almost always very far away from it? 

## "Bias Formula"

- Consider a simple linear regression $$y=\beta_0 + \beta_1 x + \varepsilon.$$
and assume we would observe $\varepsilon$.

- One can show that

  $$\hat \beta_1 - \beta_1 = cor(x,\varepsilon)\frac {sd(\varepsilon)}{sd(x)}$$
  using the sample correlations and sample standard deviations.

- This expression is an estimator of the bias of $\hat \beta_1$. (The actual bias is the expected value of it.)

- Thus essentially the bias has the same sign as the correlation between $x$ and $\varepsilon$.

*Analysis in R*

- Simulate a demand model with endogenous prices and check for your simulation that the "bias formula" above holds. 


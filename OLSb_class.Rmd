---
title: "Session 3: Introduction into R and Econometrics"
author: "Dennis Rickert, MINES Paris-PSL University"
date: "2023-07-17"
output: 
  ioslides_presentation: 
#  html_document: 
#    number_sections: yes
#    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction 

## Overview of Session 3+4

What we do in Session 3+4

- Intro: Intuitive examples of causality vs correlation
- Understanding the causal effect of price on demand
- 


## Learning objectives

- 

## Concept of Simple Regressions

Assume two variables (or vectors) $x$ and $y$ from some population (e.g., individuals, firms, regions) 

- "Explaining $y$ in terms of $x$," or 
- "studying how $y$ changes when we change $x$."


Some examples include:

- $y$ is  *market value* of a footballer and $x$ the *goals* he scored
- $y$ is the *quality of wine* and $x$ the *weather*
- $y$ is  *soybean crop yield* and $x$ is amount of fertilizer;
- $y$ is *hourly wage* and $x$ is *years of education*;
- $y$ is *crime rate* and $x$ is number of police officers.  

## The Simple Regression Model

We confront three issues, if we want to “explain $y$ in terms of $x$”

- Which functional relationship do we assume (e.g., linear)? 
- How do we allow factors, other than $x$, to affect $y$? 
- How can we ensure a causal relationship between $y$ and $x$?  
  - *ceteris paribus* notion: holding constant other factors

## To address those 3 issues, we define

$$y=\beta_0+\beta_1x+u$$

- Linear relation between observables $x$ and $y$ (*Issue 1*)
- $\beta_1:$ **slope parameter** in the relation between $y$ and $x$
- $\beta_0:$ **intercept parameter** aka the **constant term**
- Error term $u:$ Unobservables allowed to enter (*Issue 2*)
- Certain assumptions to get causal effect of $x$ on $y$ (*Issue 3*)  

Main intuition: given we observe $x$ and $y$

- $\beta_1$ and $\beta_0$: estimated making assumptions on $u$
- $\beta_1$: how changes in $x$ affect changes in $y$ given $\beta_0$ and $u$

## Using ceteris paribus notion, we get

... the causal effect of $x$ on $y$ if and only if

- the unobserved factors in $u$ are held fixed, 
- so that the change in $u$ is zero, i.e., $\Delta{u}$= 0, 

$$\Delta{y}=\beta_1\Delta{x}~~~~if~~~~\Delta{u}=0$$ 

- then $x$ has a linear effect on $y$ 
- where $\beta_1$ yields causal effect of $x$ on $y$ 

## Example: Soybean Yield and Fertilizer
  
Suppose that soybean yield per land unit is determined by:  
      
$$yield=\beta_0+\beta_1 fertilizer + u$$

where $y= yield$ and $x=fertilizer$. 

- Coefficient $\beta_1$ measures the effect of fertilizer on yield, holding unobservable factors fixed  
- Error term $u$ contains factors, such as, land quality, rainfall, ... 
- $\beta_0$ captures factors that are constant across land units

$$\Delta{yield}=\beta_1\Delta fertilizer$$

## Example: A Simple Wage Equation
  
Relating a person's wage to education and unobservables:  
          
$$wage=\beta_0+\beta_1 educ + u$$ 

- $wage$ is dollars earned per hour and $educ$ is education years, 
- $\beta_1$ measures the change in hourly wage given another year of education, holding all other factors fixed. 
- Some of the unobserved factors include 
  - labor force experience, 
  - innate ability, 
  - tenure with current employer, 
  - work ethic, 

$$\Delta{wage}=\beta_1\Delta educ$$

## The Simple Regression Model

Does the above model really allows us to draw causal conclusions about how $x$ affects $y$?

$\Delta{y}=\beta_1\Delta{x}$ implies that $\beta_1$ measures the effect of $x$ on $y$, holding all other factors (in $u$) fixed, i.e., $u$ does not affect $y$

- Is this the end of the causality issue? Unfortunately, **NO**!
- We will now learn why!


## The Simple Regression Model

How can we learn about the causal effect of $x$ on $y$, holding other factors fixed, if we ignore all those other factors?

Take the expected value of $y=\beta_0+\beta_1x+u$ conditional on $x$

$$E(y|x)=\beta_0+\beta_1x$$

We get this expression by two assumptions on error term $u$

- $E(u)=0$ (harmless)
- $E(u|x)= u = 0$ (less harmless)

$\beta_1$ is causal due to this *zero conditional mean assumption*

## Assumption $E(u)=0$

.. says nothing about the relation between $u$ and $x$, but simply makes a statement about the distribution of unobservables

We can always redefine the intercept in $y=\beta_0+\beta_1x+u$ to make $E(u)=0$ true.

  - Soybean example: we can normalize the unobserved factors affecting soybean yield, such as land quality, to 0
  - Wage example: We can assume that things such as average ability are zero in the population of all working people.

## Assumption expected value of u given x is equal to 0

or formally $E(u|x)=u=0$

- for any $x$, we can obtain the expected (or average) value of $u$ for that slice of the population described by the value of $x$.
- The crucial assumption is that the average value of $u$ does not depend on the value of $x$.
- When assumption $E(u|x)=E(u)$ holds, we say that $u$ is **mean independent** of $x$. 
- When we combine mean independence with assumption $E(u)=0$, we obtain the **zero conditional mean assumption**, $E(u|x)=0$.

## 

Let us see what equation $E(u|x)=E(u)$ entails in the wage example.

- To simplify the discussion, assume that $u$ is the same as innate ability (or intelligence).
- Then equation $E(u|x)=E(u)$ requires that the average level of ability is the same, regardless of years of education $x$.
    - For example, if $E(abil|8)$ denotes the average ability for the group of all people with eight years of education, 
    - and $E(abil|16)$ denotes the average ability among people in the population with sixteen years of education, 
    - then equation $E(u|x)=E(u)$ implies that these must be the same.
- In fact, the average ability level must be the same for all education levels.

Is this a reasonable assumption?


## 

If, for example, we think that average ability increases with years of education, then equation $E(u|x)=E(u)$ is false.

- This would happen if, on average, people with more ability choose to become more educated.
- As we cannot observe innate ability, we have no way of knowing whether or not average ability is the same for all education levels.
- But: this is an issue that we must address before relying on simple regression analysis!

## 
In the fertilizer example, if fertilizer amounts are chosen independently of other features of the land plots, then equation $E(u|x)=E(u)$ will hold: 
- the average land quality will not depend on the amount of fertilizer.

However, if more fertilizer is put on the higher-quality plots of land, then the expected value of $u$ changes with the level of fertilizer, and equation $E(u|x)=E(u)$ fails.


## 

Given the zero conditional mean assumption $E(u|x)=0$, it is useful to view equation $y=\beta_0+\beta_1x+u$ as breaking $y$ into two components.

1. The piece $\beta_0+\beta_1s$, which represents $E(y|x)$, is called the *systematic part* of $y$ - that is, the part of $y$ explained by $x$ 
2. and $u$ is called the *unsystematic part*, or the part of $y$ not explained by $x$. 

Later we discuss how to determine how large the systematic part is relative to the unsystematic part.


## The Ordinary Least Squares Estimator*

Now that we have discussed the basic ingredients of the simple regression model, we will address the important issue of how to estimate the parameters $\beta_0$ and $\beta_1$ in equation $y=\beta_0+\beta_1x+u$.

- To do this, we need a sample from the population.
- Let {($x_i,y_i):i=1,...,n$} denote a random sample of size $n$ from the
population.
- Because these data come from equation $y=\beta_0+\beta_1x+u$, we can write $y_i=\beta_0+\beta_1x_i+u_i$ for each $i$.
- Here, $u_i$ is the error term for observation $i$ because it contains all factors affecting $y_i$ other than $x_i$.

As an example, $x_i$ might be the annual income and $y_i$ the annual savings for family $i$ during a particular year.

- If we have collected data on 15 families, then _n=15_.
- A scatterplot of such a data set is given in the figure below, along with the (necessarily fictitious) population regression function.
- We must decide how to use these data to obtain estimates of the intercept and slope in the population regression of savings on income.

## Scatterplot of savings and income for 15 families, 

and the population regression $E(savings|income)=\beta_0+\beta_1 income$


```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_2.png")
```


## The Ordinary Least Squares Estimator

The most basic **estimator** is the **Ordinary Least Square (OLS) estimator**.

- The idea is to choose the parameter $\beta$ that minimizes the squared sum of the error $u_i$ of the model
- By construction, we never observe $u_i$ that is unknown to us

Instead we have the **sample regression model**: $y_i=\hat{y}_i + \hat{u}_i = \hat{\beta_0}+ \hat{\beta_1}x_{1i}+ \hat{u}_i$

- $\hat{y}_i$ are the predicted values, $\hat{\beta}$ are estimated coefficients, and $\hat{u}_i$ are residuals


## The Ordinary Least Squares Estimator

Assume the **population regression model**:  


$$y_i = \beta_{0} + \beta_{1}x_{1i} + u_i$$

The OLS estimator has the following objective function:

$$(\hat{\beta}_{0}^{OLS}, \hat{\beta}_{1}^{OLS})=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i\hat{u}_i^2]}=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i(\hat{y}_i-y_i)^2]}$$ 

The algorithm chooses $(\hat{\beta}_{0}, \hat{\beta}_{1})$ until $\hat{u}_i^2$ reaches a global minimum

## Graphical illustration OLS: Fitted Values and Residuals

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_4.png")
```


**Analytical Derivation of the OLS Estimator (Self Study)**

- An implication from assumption $E(u|x)=E(u)$ is that in the population $u$ is uncorrelated with $x$.
- Combined with $E(u)=0$ we get that the covariance between $x$ and $u$ is zero.

$$Cov(x,u)=E(xu)=0$$

- In terms of the observable variables $x$ and $y$ and the unknown parameters $\beta_0$ and $\beta_1$, the two equations $E(u)=0$ and $Cov(x,u)=E(xu)=0$ can be written as

$$E(y-\beta_0-\beta_1x)=0$$

and

$$E[x(y-\beta_0-\beta_1x)]=0$$

- The two above equations imply two restrictions on the joint probability distribution of $(x,y)$ in the population.
- Since there are two unknown parameters to estimate, we might hope that equations the two equations can be used to obtain good estimators of $\beta_0$ and $\beta_1$.


## **Derivation of the OLS Estimator (Self Study)**

- The respective _sample_ counterparts of the two above equations are

$$n^{-1}\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

and

$$n^{-1}\sum_{i=1}^nx_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

- The equation $n^{-1}\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0$ can then be rewritten as  

$$\bar{y}=\hat\beta_0+\hat\beta_1\bar{x}$$

and

$$\hat\beta_0=\bar{y}-\hat\beta_1\bar{x}$$

- Once we have the slope estimate $\beta_1$, it is straightforward to obtain the intercept estimate $\beta_0$, given $y$ and $x$.

## Derivation of the OLS Estimator (Self Study)

- Plugging the last equation into $n^{-1}\sum_{i=1}^nx_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0$ and dropping the $n^{-1}$ (because it does not affect the solution) yields  

$$\sum_{i=1}^nx_i[y_i-(\bar{y}-\hat\beta_1\bar{x})-\hat\beta_1x_i)]=0$$

 which can be rearranged to  
 
$$\sum_{i=1}^nx_i(y_i-\bar{y})=\hat\beta_1\sum_{i=1}^nx_i(x_i-\bar{x})$$

- And because

$$\sum_{i=1}^nx_i(x_i-\bar{x})=\sum_{i=1}^n(x_i-\bar{x})^2$$

and 

$$\sum_{i=1}^nx_i(y_i-\bar{y})=\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})$$


## The simple linear regression model
**Derivation of the OLS Estimator (Self Study)**

- The estimated slope therefore is  

$$\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}$$

 - This is simply the sample covariance between $x_i$ and $y_i$ divided by the sample variance of $x_i$.
 
Or:
 
$$\hat{\beta}_{1}=\frac{Cov[y,x_1]}{Var[x_1]}$$

and

$$\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1}\bar{x_1}$$

## Derivation of the OLS Estimator (Self Study)

- $\hat{\beta}_0$ and $\hat{\beta}_1$ are called the **ordinary least squares** estimators of the parameters.
- The name can be justified as follows:
- For any $\beta_0$ and $\beta_1$ define a **fitted value** for $y$ where $x$ = $x_i$ as  
    
$$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i$$

- This is the value we predict for $y$ when $x=x_i$ for the given intercept and slope.
- There is a fitted value for each observation in the sample.
- The **residual** for observation $i$ is the difference between the actual $y_i$ and its fitted value:
    
$$\hat{u}_i=y_i-\hat{y}_i=y_i-\hat{\beta}_0+\hat{\beta}_1x_i$$

-  Now, suppose we choose $\hat{\beta_0}$ and $\hat{\beta_1}$ to make the **sum of squared residuals**,

$$\sum_{i=1}^n\hat{u}_i^2=\sum_{i=1}^n(y_i-\hat{\beta}_0+\hat{\beta}_1x_i)^2$$

## Derivation of the OLS Estimator (Self study)

- The conditions necessary to minimize above equation (first order conditions) are given exactly by the two equations we introduced earlier:

$$n^{-1}\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

and

$$n^{-1}\sum_{i=1}^nx_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

- From our previous calculations we know that the solutions ot the OLS first order conditions are given by

$$\hat{\beta}_{1}=\frac{Cov[y,x_1]}{Var[x_1]}$$

and

$$\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1}\bar{x_1}$$

- The name “ordinary least squares” comes from the fact that these estimates minimize the sum of squared residuals.


## The Simple Regression Model (Summary)

We are concerned with estimating the population parameters $\beta_0$ and $\beta_1$, of the simple linear regression model  

$$y=\beta_0+\beta_1x+u$$

- from a random sample of $y$ and $x$. 

The ordinary least squares (OLS) estimators are  

$$\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{Cov(x,y)}{Var(x)}$$

$$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$

Based on these estimated parameters, the OLS regression line is  

$$\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$$


For a given sample, we just need to calculate the four statistics $\bar{y},\bar{x}, Cov(x,y)$ and $Var(x)$ and plug them into these equations. 


## Rumour between the economist wine maverick Ashenfelter and (virtually all) wine experts

<blockquote>
There are differences in price and quality of wine from year to year that are sometimes very significant: wines are tasting better when they are older. So, there is an incentive to store young wines until they are mature. Wine experts taste the wine and then predict which ones will be the best later. But it is hard to determine the quality of the wine when it is so young just by tasting it, since the taste will change significantly by the time it will be consumed.
</blockquote>


***Summary of Ashenfelter Wine Equation: The New York Times***
<blockquote> 
Calculate the winter rain and the harvest rain (in millimeters). Add summer heat in the vineyard (in degrees centigrade). What do you have? A very, very passionate argument over wine.
</blockquote>


##
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_forbes.jpg")
```

##
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_nyt.jpg")
```

## 
<iframe width="560" height="315" src="https://www.youtube.com/embed/8WMRj9mTQtI" frameborder="0" allowfullscreen></iframe>

##  Have a look at this table from the research paper, and answer the questions below. 

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_table2.jpg")
```


## Let's have a look how the magic wine equation works

1. Load the data set ('wine.csv').
2. Calculate the average price of wine. Explain why this is not a good predictor for future prices. 
3. Explain under which circumstances OLS is the preferred choice and why.  
4. Regress average temperature in the growing season on prices. What do you see? How do you interpret the slope?
5. How do you measure the "fit" of a model? Which of the two models has the better fit? 
6. And are there even better models?
7. Show if OLS assumptions are violated

## 
df2 = pd.read_csv("C:/Users/Anonymouse/Dropbox (CERNA)/Econometrics MINES/wine.csv")

```{r radio, echo = FALSE}
library(learnr)

```

## A simple linear regression model satisfies the following relationship for all observations $t=1,...,T$

$$y_{t}=\beta_{0}+\beta_{1}x_{t}+\varepsilon_{t}$$

  - $y=(y_{1},...y_{T})$ is the dependent variable.
  - $x=(x_{1},...x_{T})$ is the explanatory variable.
  - $\varepsilon=(\varepsilon_{1},...,\varepsilon_{T})$ (epsilon) is a random variable that describes unobserved influences on $y$, sometimes called *disturbance*. We will typically make some assumptions on the distribution of $\varepsilon$. We will also use the letters $u$ and $\eta$ (eta) to denote disturbances.
  - $\beta=(\beta_{0},\beta_{1})$ is the vector of true coefficients.

## [2col] Estimate, Predicted Value and Residuum

##. leftcol

- Let $\hat{\beta}$ be an *estimate* of the true parameter vector $\beta$.

- The *predicted values* (also called fitted values) of $y$ are given by $$\hat{y}=\hat \beta_0 + \hat \beta_1 x$$

- The *residuals* (estimated values of the disturbance) are given by $$\hat{\varepsilon}=y-\hat{y}=y-\hat \beta_0 - \hat \beta_1 x$$

  - The residuals $\hat{\varepsilon}$ are close to the true disturbances $\varepsilon$ if our estimate $\hat \beta$ is close to the true parameters $\beta$.


##. rightcol
```{r echo=FALSE, fig.width=3.2, fig.height=3, show_code="no", dev="svg"}
library(ggplot2)
# Preisexperiment
beta0 = 100
beta1 = 12
T = 20
set.seed(123456)
p = sample(seq(2,6, length=T+1), T, replace = FALSE)
eps = rnorm(T,0, 20)
D = round(beta0 - beta1*p + eps)
d = data.frame(t=seq_along(p),D=D,p=p,eps=eps)

d$y_hat = fitted(lm(data=d, D~p))
dc = d[9,]

ggplot(data=d,aes(x=p,y=D, label=t)) +
  geom_point(alpha=0.5) +
  geom_smooth(method=lm, color="red", se=FALSE) +
  geom_segment(data=dc, aes(x=p,xend=p,y=y_hat,yend=D),col="black") +
  geom_point(data=dc,color="black", size=2) +
  geom_point(data=dc,aes(y=y_hat),color="red", size=2) +
  annotate('text', x=dc$p, y=dc$D+7, color="black", size=5,  label="y[i]", parse=TRUE)+  
#  annotate('text', x=4.1, y=68, color="black", size=5,  label="hat(eps)[i]", parse=TRUE)+  
  annotate('text', x=dc$p, y=dc$y_hat-7, color="black", size=5,  label="hat(y)[i]", parse=TRUE)+  
  xlab("x") +
  ylab("y") +
  theme_bw()


```

## Ordinary Least Squares Estimation (OLS)

- An ordinary least squares (OLS) estimate minimizes the sum of squared residuals $$\hat{\beta}=\arg\min\sum_{t=1}^{T}\hat{\varepsilon}_{t}^{2}$$

- For the simple linear regression (one explanatory variable), the OLS estimator $\hat \beta_1$ has the following formula
  $$\begin{aligned}
    \hat \beta_1 &= \frac{Cov(x_t,y_t)}{Var(x_t)} =
    cor(x_t,y_t) \frac{sd(y)}{sd(x)}
  \end{aligned}$$
  where $cor$ denotes an empirical correlation and $sd$ an empirical standard deviation for our sample data. 

## Linear Regression Model in Matrix Notation

- One often writes a linear regression model in matrix notation:
  $$y=X\beta+\varepsilon$$ 
  with
  $$X=\left(\begin{array}{cccc}
    1 & x_{1}\\
    ... & ...\\
    1 & x_{T}
    \end{array}\right)=\left(\begin{array}{cccc}
    \boldsymbol{1} & x
  \end{array}\right)$$

- The OLS estimator $\hat \beta = (\hat \beta_0, \hat \beta_1)$ is then given by
$$\hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y$$
- You don't have to understand the matrix notation for this lecture, but for those who do, we will sometimes mention it.

## Estimators and estimates

- Since $y=X\beta+\varepsilon$, the OLS estimator can be rewritten as

  $$\begin{aligned}
  \hat{\beta} & =(X^{\prime}X)^{-1}X^{\prime}y\\
   & =(X^{\prime}X)^{-1}X^{\prime}(X\beta+\varepsilon)\\
   & =\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\end{aligned}$$

  - This means $\hat \beta$ is a linear transformation of the true parameters $\beta$ and the disturbance $\varepsilon$

- As a function of a random variable $\varepsilon$ the OLS *estimator* $\hat{\beta}$ is itself a random variable

- The OLS *estimate* $\hat{\beta}$ is a realization of the OLS estimator, i.e. the value for particular draws of $\varepsilon$ and $X$.

- To understand what econometrics and most of statistics is doing, one should keep in mind that *an estimator is a random variable*.

## A Monte-Carlo Simulation of \(\hat \beta\) in R

*Analysis in R* (R analysises would be shown life in the lecture and I made videos for some. You will do similar analysis yourself in an RTutor problem set)

- Start with the simulation of our ice cream model from Chapter 1a with random prices and estimate the demand function.

- Write an R function with name `sim.and.est` that performs this simulation and returns the estimated coefficients $\hat \beta$ of the demand function. Your function shall have the sample size $T$ as an argument.

- Repeat the simulation and estimation several times (draw new $\varepsilon$ each time) and compute and store the OLS estimates. Use the function `simulation.study` in the package `sktools` to conduct a systematic simulation study. Plot the distribution of the resulting estimates $\hat{\beta}_{1}$ and compare it with the true value of $\beta_{1}$. How does the distribution change if we change the sample size $T$?

#. frame Distribution of our estimator \(\hat \beta_1\)

We found in our Monte-Carlo simulation study that the estimator $\hat \beta_1$ has a distribution, that depends on the sample size $T$. Here it is shown for $T=20$ and a true $\beta_1=-1$

![image](figures/sim1b.svg)

## Standard Error of OLS estimator

- In a simple linear regression (one explanatory variable) $$y= \beta_0 + \beta_1 x + \varepsilon$$ where the $\varepsilon$ are independently, identically normal distributed, the standard deviation of the OLS estimator $\hat \beta_1$ can be *estimated* by

$$se(\hat \beta_1) = \hat{sd}(\hat \beta_1) = \frac{1}{\sqrt T} \frac{sd(\hat \varepsilon)}{sd(x)}$$
- We call this estimate of the standard deviation the *standard error* of $\hat \beta_1$.

- Observations: We can estimate $\beta_1$ more precisely if we have...

  - a larger the sample size $T$
  
  - more variation in $x$ (higher standard deviation).

*Analysis in R:* We run a linear regression with `lm` and call `summary` on the result to see besides the estimated coefficients also the standard errors.

## Robust Standard Errors

- There is also a matrix formula to compute the standard errors for all $\hat \beta$ that can also be used for multiple linear regressions with more than one explanatory variable.

- If the $\varepsilon$ are not identically, independently normal distributed, one should use approbriate *robust* standard errors. Most empirical papers in economics use some robust standard errors.

- We don't explain robust standard errors further in this course. Just note that in R a convenient way to use robust standard errors is the function `lm_robust` in the package `estimatr` or the function `felm` in the package `lfe`.

## Criteria for estimators: Bias

- **Bias:** Recall that an estimator $\hat{\beta}$ is a random variable since it depends on the realizations of $\varepsilon.$ Let $E\hat{\beta}$ be the expected value of $\hat{\beta}$. The bias of $\hat{\beta}$ measures a systematic over- or underestimation of $\hat{\beta}$ compared to $\beta$: $$Bias(\widehat{\beta})=E\hat{\beta}-\beta.$$

- **Unbiasedness:** An estimator $\hat{\beta}$ is unbiased if its Bias is 0, i.e. $$E\hat{\beta}=\beta$$

## Criteria for estimators: Standard Deviation

- For two unbiased estimators of $\beta_i$, one would typically prefer an estimator with a lower standard deviation $sd(\hat{\beta}_{i})$ (or equivalently the one with the lower variance $Var(\hat{\beta}_{i})$)

## Criteria for estimators: Mean Squared Error

- **Mean squared error**: The mean squared error of $\hat{\beta}_{i}$ is given by

    $$\begin{aligned}
    MSE(\hat{\beta}_{i}) & =E(\hat{\beta}_{i}-\beta_{i})^{2}\\
     & =Bias(\hat{\beta}_{i})^{2}+Var(\hat{\beta}_{i})\end{aligned}$$


- *Analysis in R:* We go back to the previous simulation study in R and compute an approximation to the bias and mean squared error of $\hat{\beta}_{0}$ by analysing the simulated distribution of $\hat{\beta}_{0}$. How does the MSE change when you increase the number of observations $T$ in the simulated data set?

## Criteria for estimators: Consistency

- An estimator $\hat{\beta}$ is (strongly) **consistent** if its MSE converges to 0 as the sample size $T$ grows large $$\lim_{T\rightarrow\infty}MSE(\hat{\beta})=0.$$

- A note for the mathematic students: Strong consistency implies weak consistency, which means that the estimated parameters $\widehat{\beta}$ converges (in probability) to the true parameters $\beta$ $$\underset{T\rightarrow\infty}{\mbox{plim}}\widehat{\beta}=\beta$$

- **Consistency is often seen the most important requirement for an estimator.**

- If an estimator is inconsistent that is typically because it is biased and the bias does not go away as $T\rightarrow\infty$.

## Criteria for estimators: Efficiency

- An estimator $\hat{\beta}$ is **efficient** (within a specified class of estimators) if there is no other estimator that has a lower mean squared error.

- We won't discuss efficiency deeper in this course.

## Assumptions of the simple linear regression model

- We now state a series of assumptions for the simple linear regression model (one explanatory variable).

  - A1: $E(\varepsilon_t|x)=0$

  - A2: The $\varepsilon_{t}$ are identically and independently distributed.

  - A3: The $\varepsilon_{t}$ are normally distributed

  - A1-A3 are often compactly written as $\varepsilon_{t}\overset{i.i.d.}{\sim}$ $N(0,\sigma^{2}).$

  - A4: The explanatory variable $x$ must have positive variance and be deterministic or a stationary random variable. (We don't discuss what stationary means in this course, but you can look it up on Wikipedia.)

- If all assumptions are satisfied, the OLS estimator $\hat{\beta}$ will be consistent, unbiassed and efficient.

- Good intuitive overview: “An Introduction to Econometrics” by Peter Kennedy

## Assumption A1

- **A1** No matter which values of $x$ we observe, the conditional expected value of $\varepsilon_{t}$ is always zero: $$E(\varepsilon_{t}|x)=0$$

- The important thing is not the 0 on the right. If it were a positive or negative value, we could always redefine the constant $\beta_0$ to make it 0.

- The important thing is that the expected value of $\varepsilon_t$ does not depend on $x$. This means knowing $x$ shall give us no information about the expected value of $\varepsilon_t$.
  
- In our ice cream example with profit maximizing prices this condition is violated. Higher demand shocks lead to higher prices. This means if we observe a high price, we expect that there was a positive demand shock $\varepsilon_t$.   

## Exogenous and Endogenous Variables

- We say the explanatory variable $x$ is **exogenous**, if it is uncorrelated with $\varepsilon$ $$\mbox{cor}(x_t,\varepsilon_t)=0$$

- We say $x$ is **endogenous** if $\mbox{cor}(x_t,\varepsilon_t)\ne0$

- Condition A1 $E(\varepsilon|x)=0$ can only be satisfied if $x$ is exogenous.

- We will typically just check whether $x$ is exogenous, even though A1 is a stronger condition. A1 is sometimes called *strong exogeniety*.  In all examples studied in this course, exogeniety of $x$ implies that also A1 holds.

## The problem of endogeniety

- **A1 is the most important assumption:**

  - **If $x$ is endogenous, the OLS estimator $\hat{\beta}$ will (typically) be inconsistent and biased.**

- We typically check whether there is endogeniety as follows:

  - Think of a true model for the data generating process; specify which factors are part of the random shock $\varepsilon$

  - See if in that model the explanatory variable $x$ is a function of $\varepsilon$ or of some unobserved variable that is part of $\varepsilon$. If that is the case $x$ is endogenous.

## Exploring Endogeniety in R

*Analysis in R*: We consider the ice cream model from slides 1a. 

- We compare the two cases that prices are randomly drawn and prices are chosen optimally. In which model is A1 satisfied / violated and the OLS estimator consistent / inconsistent? We compute the correlation between $\varepsilon$ and $p$ in your simulated data.

- For real world data, we never observe $\varepsilon$, i.e. we cannot simply see in the data that there is an endogeniety problem. If we compute the correlation between $p$ and $\hat{\varepsilon}$ (the OLS residual), we see that it is always 0 (or due to rounding errors maybe slightly away) even if $\varepsilon$ and $p$ are correlated.

- We now assume prices $p_{t}$ are simply always set 10% above the cost $c_{t}$ (and costs shall be uncorrelated with $\varepsilon$). Are prices $p_{t}$ then endogenous or exogenous? Is the OLS estimator consistent?

- We now consider the model in which prices are chosen optimal. Consider in your simulation the limit case that $\sigma_{\varepsilon}\rightarrow 0$. What is then the main source of variation in prices? To which value will $cor(\varepsilon,p)$ converge? We study in the simulation whether the OLS estimator of prices seems to be consistent and unbiased in this limit case.

## A2, No auto-correlation and no heteroskedasticity

- **A2** The $\varepsilon_{t}$ are identically and independently distributed.

- Typical violations of A2:

  - auto-correlation: demand shocks may be persistent across periods

  - heteroskedasticity: the variance of $\varepsilon_{t}$ can depend on the explanatory variable (this alone does not yet mean that A1 is violated)

- A2 is moderately important. If violated, the OLS estimator $\hat{\beta}$ is still consistent but not efficient. One must calculate standard errors using an appropriate formula for robust standard errors.

- We don't study violations of A2 in this course.

## A3: Normally distributed disturbances

- **A3** $\varepsilon_{t}$ is normally distributed

- It is nice if A3 holds, but it is not crucial. Even if A3 is violated, the OLS estimate $\hat{\beta}$ is the best unbiased linear estimators of $\beta$ (Gauss-Markov Theorem). Significance tests would only be asymptotically correct. If A1-A3 (and the other assumptions) holds, $\hat{\beta}$ coincides with Maximum Likelihood estimator and is efficient.

## frame 95% Confidence Intervals

- If assumptions A1 holds (no endogeniety problem) then with approximately 95% probability we find an estimate $\hat{\beta}_{i}$ such that the interval of plus-minus 2 standard errors around $\hat{\beta}_{i}$ contains the true parameter $\beta_{i}$. We call this interval $$[\hat{\beta}_{i}-2\cdot se(\hat{\beta}_{i})\;;\;\hat{\beta}_{i}+2\cdot se(\hat{\beta}_{i})]$$ the approximate **95% confidence interval**.

*Analysis in R* 

1. Run a linear regression on some simulated data without endogenitey problem and apply the R function `summary` on the result to see the estimated standard errors. Compute in your head the approximate 95% confidence interval and also use the function `confint` for the exact confidence interval.

2. Now simulate data with an endogeniety problem. Does the true parameter $\beta_{1}$ still typically lie within the 95% confidence interval around $\hat{\beta}_{1}$ or can it be that it is almost always very far away from it? 

## "Bias Formula"

- Consider a simple linear regression $$y=\beta_0 + \beta_1 x + \varepsilon.$$
and assume we would observe $\varepsilon$.

- One can show that

  $$\hat \beta_1 - \beta_1 = cor(x,\varepsilon)\frac {sd(\varepsilon)}{sd(x)}$$
  using the sample correlations and sample standard deviations.

- This expression is an estimator of the bias of $\hat \beta_1$. (The actual bias is the expected value of it.)

- Thus essentially the bias has the same sign as the correlation between $x$ and $\varepsilon$.

*Analysis in R*

- Simulate a demand model with endogenous prices and check for your simulation that the "bias formula" above holds. 


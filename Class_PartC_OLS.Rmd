---
title: "Session 3+4: Introduction into R and Econometrics"
author: "Dennis Rickert, MINES Paris-PSL University"
date: "2023-07-17"
output: 
  ioslides_presentation: 
#  html_document: 
#    number_sections: yes
#    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Session 3+4 - Part C: Refresher of Econometrics (OLS)

## Intro

We start with two simple examples that outline

- relevant research questions in economics
- the interest in establishing causal relations 
- when we can use simple regressions for causality

## Example 1: More money by education?

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("College_NewYorker.jpg")
```

## Example 2: What affects wine prices?

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_forbes.jpg")
```


## What we do in Session 3+4: Part C

So far we learned  

- difference between correlation and causality
- to represent economic relations using graphs and regressions 
- omitting a variable may change sign of a relation 

Here we learn 

- the formal concept  of simple regression (OLS)
- that OLS is causal as long as *ceteris paribus* holds
- graphical intuition of the OLS estimator 
- how to interpret coefficients and interpret the "fit"

## Some words before we start

First we do some nitty-gritty formulas

- We need some theory in order to
  - make our assumptions transparent
  - derive testable hypotheses
  - avoid bad research

<br>

But do not forget why we do it

- answer fun research questions (education, wine, ice cream) 

And I will use graphs for illustration whenever possible

## Concept of Simple Regressions

Two variables $y$ and $x$ from a population (e.g., clients, firms)

- "explaining $y$ in terms of $x$," or 
- "studying how $y$ changes when we change $x$."

<br>

Some examples include:

- $y$ is the *demand* of ice cream and $p$ is the price
- $y$ is the *price of wine* and $x$ the *weather*
- $y$ is *hourly wage* and $x$ is *years of education*
- $y$ is  *market value* of a footballer and $x$ the *goals* s/he scored

## The Simple Regression Model

***Three issues*** if we want to “explain $y$ in terms of $x$”

- Which functional relationship do we assume (e.g., linear)? 
- How do we allow factors, other than $x$, to affect $y$? 
- How can we ensure a causal relationship between $y$ and $x$?  
  - *ceteris paribus* notion: holding constant other factors

## To address those 3 issues, we define

$$y=\beta_0+\beta_1x+u$$

where 

- $\beta_0$: **intercept parameter** aka the **constant term**
- $\beta_1$: **slope parameter** in the relation between $y$ and $x$

and

- Linear relation between observables $x$ and $y$ (*Issue 1*)
- Error term $u$: Unobservables allowed to enter (*Issue 2*)
- Assumptions on $u$ to get causal effect of $x$ on $y$ (*Issue 3*)  

## How to get causality?

$$y=\beta_0+\beta_1x+u$$
<br>

Main intuition for causality: given we observe $x$ and $y$

- estimate $\beta_1$ and $\beta_0$ making assumptions on $u$
- $\beta_1$: how changes in $x$ affect changes in $y$ 
  - given $\beta_0$ and $u$

## We get the causal effect of $x$ on $y$ 

Ceteris paribus: we hold the unobserved factors in $u$ fixed

- so that the change in $u$ is zero (no effect on $y$) 
- i.e., $\Delta{u}$= 0, 

$$\Delta{y}=\beta_1\Delta{x}~~~~if~~~~\Delta{u}=0$$ 

- then $x$ has a linear effect on $y$ 
- where $\beta_1$ yields causal effect of $x$ on $y$ 
- because constant $\beta_0$ cancels out $\Delta \beta_0=0$ 



## Example: A Simple Wage Equation
  
Relating a person's wage to education and unobservables:  
          
$$wage=\beta_0+\beta_1 educ + u$$ 

- $wage$ is dollars earned per hour and $educ$ is education years, 
- $\beta_1$ measures the change in hourly wage given another year of education, holding all other factors fixed. 
- Some of the unobserved factors include 
  - labor force experience, 
  - innate ability, 
  - tenure with current employer, 
  - work ethic, 

$$\Delta{wage}=\beta_1\Delta educ$$



## Example: Bob's Ice Cream Business

Similarly, $\beta_1$ measures the causal effect of price on quantity

$$quantity=\beta_0+\beta_1 price + u$$

if we hold unobservable factors in $u$ fixed, 

- e.g., seasonality has no systematic impact on prices
- similarly for advertising and quality

<br>

Only then we get the causal effect: 

$$\Delta{quantity}=\beta_1\Delta price$$

## Two questions you may ask yourself

Is this the sufficient to obtain causality? No! 

How can we learn about the causal effect of $x$ on $y$, holding other factors fixed, if we ignore all those other factors?

- We can only do so if a certain assumption holds
- This assumption is called: Zero conditional mean assumption

<br> 

We use now the fance terminology: expected value

- often expected value is just the average of our sample
- so just think of average value when I say expected value


## Zero conditional mean assumption

We assume: $\mathbb{E}[u|x]=\mathbb{E}[u]=0$

- the average of $u$ needs to be the same for all slice of $x$
- which we normalize to $0$ for convenience
- we can also say that $cor(u,x)=0$

Example: Bob's ice cream business.  

- if we observe $x=prices$ are high, then average demand shock $u$ is the same as if we observe $x=prices$ are low
- where we normalize the average $u=0$: demand shock is zero for high price and for low price
- But we know: given prices are high, we more likely are in a state of high demand


## Zero conditional mean assumption

To see why we get causality, take the expected value of $y|x$.   

- What is the average value of $y$ for each slice of the data $x$?

<br>

Then: $\mathbb{E}[y|x]= \mathbb{E}[\beta_0 +\beta_1x + u|x] = \beta_0 +\beta_1x + \mathbb{E}[u|x]$

<br>

which yields

$$\mathbb{E}(y|x)=\beta_0+\beta_1x$$

$\beta_1$ is causal due to this *zero conditional mean assumption*


## What does this actually imply?

Zero conditional mean assumption $\mathbb{E}[u|x]=\mathbb{E}[u]=0$ implies

$$\mathbb{E}(y|x)=\beta_0+\beta_1x$$

We get this expression by two assumptions on error term $u$:

- $E(u)=0$ (harmless)
- $E(u|x)= u$ (less harmless)

- When assumption $E(u|x)=E(u)$ holds, we say that $u$ is **mean independent** of $x$. 
- When we combine mean independence with assumption $\mathbb{E}(u)=0$, we obtain the **zero conditional mean assumption**, $E(u|x)=0$.

## $u$ is equal to 0 on average 

Harmless assumption: $E(u)=0$

... says nothing about the relation between $u$ and $x$, but simply makes a statement about the distribution of unobservables

We can always redefine the intercept in $y=\beta_0+\beta_1x+u$ to make $\mathbb{E}(u)=0$ true.

- we do not care about the causal relation of the constant $\beta_0$
- if $u=2$, then we get the same values of $y$ with $\beta_0-2$.  
- and $\beta_1x$ remains the same

Ice cream: we can normalize the unobserved factors affecting demand in $u$, such as season, to 0 without changing $\beta_1x$



## Less harmless: mean independence

or $\mathbb{E}(u|x)=u$ 

- for any $x$, we can obtain the expected (or average) value of $u$ for that slice of the population described by the value of $x$.
- The crucial assumption is that the average value of $u$ does not depend on the value of $x$.


## Wage example: $u=abil,x=educ$

Equation $\mathbb{E}(u|x)=\mathbb{E}(u)$ requires that the average level of ability $u$ is the same, regardless of years of education $x$.
  
- $E(abil|8)$: average ability for all with 8 years of education, 
- $E(abil|16)$: average ability for all with 16 years of education, 
- $E(u|x)=E(u)$ implies that $E(abil|8)=E(abil|18)$

Thus, the average ability the same for all education levels.

## Is this a reasonable assumption?


If, for example, we think that average ability increases with years of education, then equation $E(u|x)=E(u)$ is false.

- This would happen if, on average, people with more ability choose to become more educated.
- As we cannot observe innate ability, we have no way of knowing whether or not average ability is the same for all education levels.
- But: this is an issue that we must address before relying on simple regression analysis!

## Bob's ice cream (more formally) 

For equation $\mathbb{E}(u|x)=\mathbb{E}(u)$ with

- $x=price$
- $u=seasonality$

it must hold $\mathbb{E}(summer|price)=\mathbb{E}(winter|price)$

- i.e., $u=seasonality$ is uncorrelated to $x=prices$ 
- remember higher demand in summer leads to higher prices
  - observing a high price, we expect a high demand shock
  - observing a low price, we expect a low demand shock



## The Ordinary Least Squares Estimator

How to estimate the parameters $\beta_0$ and $\beta_1$ in equation 

$$y=\beta_0+\beta_1x+u$$

- To do this, we need a random sample from the population.
- Let {($x_i,y_i):i=1,...,n$} denote a sample of size $n$.
- We can write $y_i=\beta_0+\beta_1x_i+u_i$ for each $i$.

As an example, $x_i$ might be the annual income and $y_i$ the annual savings for family $i$ during a particular year.

- If we have collected data on 15 families, then _n=15_.


## Savings and income for 15 families

aim is to get: $\mathbb{E}(savings|income)=\beta_0+\beta_1 income$


```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_2.png")
```

##  OLS Graph: Fitted values and residuals

For each $x$, we can decompose $y$ into fitted value and residual. 

- Idea: choose $(\beta_0,\beta_1)$ to minimizes the squared sum of $u_i$

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_4.png")
```

(NB: we square because the sign of $u$ does not matter)

## The Ordinary Least Squares Estimator

The OLS estimator has the following objective function:

$$(\hat{\beta}_{0}^{OLS}, \hat{\beta}_{1}^{OLS})=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i\hat{u}_i^2]}=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i(\hat{y}_i-y_i)^2]}$$ 

Algorithm chooses $(\hat{\beta}_{0}, \hat{\beta}_{1})$ until $\hat{u}_i^2$ reaches global minimum

- $\hat{y}_i$ are the predicted values, 
- $\hat{\beta_0}$ and $\hat{\beta_1}$ are estimated coefficients, 
- $\hat{u}_i$ are residuals


## OLS: how to estimate $\beta_0$ and $\beta_1$?

The ordinary least squares (OLS) estimators are  

$$\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{Cov(x,y)}{Var(x)}$$

$$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$

Based on these estimated parameters, the OLS regression line is  

$$\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$$

Given observations $x_i$ and $y_i$, we just need to calculate 

- the mean of $\bar{y},\bar{x}$
- more commonly $Cov(x,y)$ and $Var(x)$. 


## Next steps

Now that we know the intuition behind OLS, we 

- revisit the fit of a model 
  - how well do we predict?
- look at hypothesis testing
  - how confident can we be that our estimates are correct?
- practice what we learned using an applied example

## Model fit: SST, SSE, SSR

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("SST.jpg")
```

## Model fit

In a simple linear regression model, the sums of squares satisfy the equation: SST = SSR + SSE.

- SSE: This is the sum of the squares of the residuals. It measures the unexplained variability by the model.
- SST: This is the total sum of squares, which measures the total variability in the dependent variable, price.
- SSR: This is the sum of squares due to regression, which measures the variability explained by the regression model. It is calculated as the difference between SST and SSE.



## Coefficient of determination $R^2$

Two interpretation: 

- How much of the variance of $y$  is explained by variance of $x$?

- How much of our model is explained by the regression line? 

Formula

$$R^2=\frac{SSE}{SST}$$

## Root Mean Squared Error (RMSE) 

a commonly used metric to evaluate the prediction error 

- the square root of the mean of the squared residuals.
  - We square each residual, 
  - take the average of the squared residuals 
  - and then take the square root of that average

- RMSE is a useful measure because it's in the same units as the dependent variable and lower values indicate better fit to the data.

## Hypothesis Testing

The coefficients in a linear regression model represent the change in the response variable for a one-unit change in the corresponding predictor variable, assuming all other variables are held constant.

For each coefficient, we test the null hypothesis that the true coefficient value is zero, against the alternative hypothesis that the coefficient is not zero.

## Hypothesis Testing

The null and alternative hypotheses for a particular regression coefficient (let's call it $\beta_1$) are often stated as follows:

- Null Hypothesis ($H0$): $\beta_1 = 0$ 
  - (The predictor has no effect on the outcome)

- Alternative Hypothesis ($Ha$): $\beta_1 \neq 0$ 
  - (The predictor has an effect on the outcome)

In essence, we test whether there is enough evidence to conclude that $x$ has an effect $y$.


## Crucial role of standard errors

In linear regression, standard errors is used to construct confidence intervals for the coefficients or to test hypotheses

Using standard errors, we can construct the following statistics 

- confidence intervals
- t-test
- p-value
- F-test

which are related. Here we just look at p-values and t-Test

## Standard Errors

It's a measure of the precision of your estimate. 

- It represents the typical amount that our estimate varies across different random samples due to sampling error. 
- Smaller standard errors mean you have more precise (i.e., less variable) estimates.

If we draw random samples from a population and estimate our parameter of interest $N$ times, 

- then standard error is measured by the standard deviation of the estimated parameters  



## Coefficient divided by standard error   

This is known as the t-Test

- used to test if a coefficient significantly different from 0 
- The test statistic is the coefficient estimate divided by its standard error
- Reject the null hypothesis $\beta_1=0$ which means no effect 
  - with high values, i.e., estimated coefficient needs to be sufficiently larger than the error that we make 
  - Rule of thumb: result significant with 95\% confidence if coefficient divided by standard error is larger than 2

## p-values

The p-value is a measure of the evidence against the null hypothesis provided by the data. 

- The p-value is the probability that the results of your test occurred at random. 
- If p-value is small (less than 0.05), we reject the null hypothesis.

A p-value of less than 0.05 corresponds to the rule of thumb above for the t-Test


## p-values

If the p-value is $<0.05$ it does not mean that your result is true. 

It means that you reject the null hypothesis (i.e., it is not 0) with a ***certain probability***. 

- naive (and wrong) interpretation: 
  - if you draw a random sample, then it your estimate would be correct 95% of the time and 5% not correct
- better (and correct) interpretation: 
  - if you fix your parameter at the estimated value, then 95\% of the estimated confidence intervals include the estimated parameter

## 

This statistic is wildly abused   

## p-hacking
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("FINALLYsignificant.png")
```

## Can people predict the future?

```{r , echo=FALSE, out.width = "20%", fig.align = "center"}
knitr::include_graphics("bem.png")
```

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("ESPisREAL.png")
```

## 

The 2011 article "*Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect*" offered statistical evidence for precognition and the psi effect.

- In other words: proof that people can see into the future
- published in reputable *Journal of Personality and Social Psychology* 
- Appeared wildly on media, such as NBC, NYT, and the [Colbert Report](www.cc.com/video/bhf8jv/the-colbert-report-time-traveling-porn-daryl-bem)


## From the Experiment Instructions:

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("BEM_instructions.png")
```

## **Three categories:**
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("bem_cat.png")
```

## For each of the categories 

Participants guessed where a certain picture would appear

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("bem_exp.png")
```

## If students guessed correctly, 

it was considered as a hit

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("bem_exp_res.png")
```

## Now you: 

To claim that people can predict the future, 

- what needs to be the percentage of correct guesses?

## Let us look at the results
```{r , echo=FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("Bem_Results.png")
```

<br>

Although this effect seems statistically significant, we should be suspicious 

- because the effect size is very low (close to null $H_0=50\%$, in the null would be $H_0=0$)
- and he did a mistkake in calculating the standard errors


## This is the full article from before

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("ESPisREAL_broken.png")
```

<br>

The very same author also proved that 

- studying after an exam can improve your grades!

## My main concerns I: Researchers' Degrees of Freedom (DoF) 

Students were shown three categories: neutral, negative, porn 

 - Cherry picking: Only significant effect for porn category (53%)
 - Low effect size (not economically significant): 
  - "extraordinary claims require extraordinary evidence"
 - False positives: Results could have been obtained by chance
 - The more Researchers DoF, the higher the probability of false positives  (results that indicate a significant result by chance)

## My main concerns II:  HARKing

Hypothesis after Results Known (HARK): 

- Ex post explanation for why there is only an effect for porn

<br>

Imagine you have coins with different colours (red, blue, green) 

- You let people guess if the coin lands on heads or tail
- Only for red you find that people get it right 53\% whether it is heads or tail (statistically significant).
  - but not for blue and green
- Your explanation is that red has always been a magic colour because blood is red, bla, bla, bla. 


# Simple OLS regression applied to the case of Bordeaux wine prices

## Background on Bordeaux wine prices 

There are differences in price and quality of wine over time: 

- wines are tasting better when they are older. 
- incentive to store young wines until they are mature. 
- Wine experts taste the wine and then predict future prices based on taste 

Can you really predict prices of mature wine based on taste of young wine?

- the taste will change significantly over time 

Ashenfelter challenged "experts" using his Econometrics skills 

## Maverick Ashenfelter made it into NYT

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_nyt.jpg")
```

- Calculate the winter rain and the harvest rain (in millimeters). 
- Add summer heat in the vineyard (in degrees centigrade). 
- What do you have? 
- A very, very passionate argument over wine.



## 
<iframe width="560" height="315" src="https://www.youtube.com/embed/8WMRj9mTQtI" frameborder="0" allowfullscreen></iframe>

##  Look at this table from the paper 

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_table2.jpg")
```


## Answer the following question

What predicts the prices of older, mature wines?

- Dry Summers, 
- Humid Summers,
- Cold Summers

## Answer the following question

What is the relationship between wine price and rain in August?

- Positively correlated + statistically insignificant 
- Positively correlated + statistically significant 
- Negatively correlated + statistically insignificant
- Negatively correlated + statistically significant

## Answer the following question
Which independent variable has the largest effect on the price of Bordeaux wine? Just evaluate the magnitude of the effect

- Age of the vintage
- Average temperature in growing season
- Rain in August
- Average temperature in September

<br>
(we look later at t-values and p-values for statistical significance)

## Answer the following question
With every additional year, the price of a vintage increases by

- 0.024% 
- 0.048%
- 2.4%
- 4.8%

## Answer the following question
The famous Ashenfelter Wine Equation can be represented as:                                      

$$
\begin{aligned}
+ & 0.024~~~~Age \\
+ & 0.620~~~~Avg. Temp. ~(growing~season) \\
– & 0.004~~~~August~rain \\
+ & 0.001~~~~Pre-vintage~rain \\ 
+ & 0.008~~~~Temp. (September)
\end{aligned}
$$

## Answer the following question

It is a good example for regressions because wine prices have an effect on weather but not weather on wine prices

- Yes 
- No 
- Maybe


## Answer the following question

Concluding from the question above, we do not have to care about unobserved confounding factors

- Yes 
- No
- Maybe

## Solutions

1. Dry Summers
2. Negatively correlated + statistically significant
3. Average temperature in growing season
4. 2.4\%
5. No
6. No
7. No

## Behind the magic wine equation

1. Load the data set ('wine.csv').
2. Regress average temperature in the growing season on prices. What do you see? 
3. Plot your regression line against the observations. How do you interpret the slope?
3. Calculate OLS by hand using the formula from class
4. How do you get the predicted price and the residuals?
5. Calculate SST, SSE, and SSR by hand
6. Calculate the R2 by hand
7. Re-run the regression with relevant controls. Which to add? 
8. How do you measure the "fit" of a model? 
9. How to you choose between models? Which is the best model?

## Load Data + first regression 

Regression of Average Temperature in Growing Season on Price 
```{r, echo=TRUE}
library("rio")
df <- import("wine.csv")
y <-df$Price
x <-df$AGST

fit <- lm(y ~ x)
fit
```

## 
```{r, echo=TRUE}
plot(x, y, main = "Regression of AGST on Price",
     xlab = "AGST (Average Temperature in Growing Season)", ylab = "Prices",
     pch = 19, frame = FALSE)
abline(lm(y ~ x, data = df), col = "blue")
```

## 
```{r,echo=TRUE,message=FALSE}
# More Detailed results
summary(fit)
```

## OLS by hand
```{r, echo=TRUE,message=FALSE}
# Load necessary library
library(dplyr)

# Calculate means
mean_price <- mean(y)
mean_AGST <- mean(x)

# Calculate the parts of the formula for Beta
beta_numerator <- sum((y - mean_price) * (x - mean_AGST))
beta_denominator <- sum((x - mean_AGST)^2)

# Calculate Beta (slope)
beta <- beta_numerator / beta_denominator

# Calculate Alpha (intercept)
alpha <- mean_price - beta * mean_AGST
```

## Print the coefficients
```{r,echo=TRUE}
# Print the coefficients
cat("Alpha: ", alpha, "\n")
cat("Beta: ", beta, "\n")
```

##  Calculate SST, SSE, SSR
```{r,echo=TRUE}
# Using the previously calculated alpha and beta

# Predicted values
predicted_price <- alpha + beta * df$AGST

# Residuals
residuals <- df$Price - predicted_price

# Sum of Squares Error (SSE)
SSE <- sum(residuals^2)

# Total Sum of Squares (SST)
SST <- sum((df$Price - mean_price)^2)

# Sum of Squares Regression (SSR)
SSR <- SST - SSE
```

## Printing our results
```{r,echo=TRUE}
# Print the sums of squares
cat("SSE: ", SSE, "\n")
cat("SST: ", SST, "\n")
cat("SSR: ", SSR, "\n")
```


## R2
```{r, echo=TRUE}
# Calculate R-squared
R_squared <- SSR / SST

# Print the R-squared
cat("R-squared: ", R_squared, "\n")
```
An R-squared between 0 and 1 indicates the extent to which the dependent variable is predictable.

- R-squared of 0.20 means that 20 percent of the variable price, can be explained by the variable AGST.



## How to measure the "fit" of a model?

Every model makes errors.These residuals are sometimes used to measure how good is a model and to compare models against each other.

Goodness if fit: measured by explained part of our model in relation

- R2: the fraction of explained variation and total variation 

$$R2=SSR/SST$$


## 
For now, we only regress prices on average temperature in the growing season

What else can we include into the model?

```{r}
head(df)
```

## Now we run the following 

```{r,echo=TRUE,results='hide'}
fit<-lm(Price ~ AGST + WinterRain + HarvestRain + Age, data=df) 
summary(fit)
```

## 
```{r,echo=FALSE}
fit<-lm(Price ~ AGST + WinterRain + HarvestRain + Age, data=df) 
summary(fit)
```

## Model evaluation

Is this model better than the one with only one variable?

## Robert Parker, the world’s most influential wine expert said:


```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("RobParker.jpg")
```


## Ashenfelter responds

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("Ashenfelter.jpg")
```

## So who is right?

Years ago, many tasters criticized Prof. Ashenfelter when he announced that his data indicated the 1986 vintage is average, 

- despite the belief of many tasters the wine would age incredibly well. 

History has supported his claims. 

- Today, many people consider the 1986 vintages to be average wines.


## There is even an app now
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("shinyapp.jpg")
```

- 

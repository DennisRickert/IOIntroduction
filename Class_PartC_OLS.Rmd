---
title: "Session 3+4: Introduction into R and Econometrics"
author: "Dennis Rickert, MINES Paris-PSL University"
date: "2023-07-17"
output: 
  ioslides_presentation: 
#  html_document: 
#    number_sections: yes
#    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Session 3+4 - Part C: Refresher of Econometrics (OLS)


##
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("College_NewYorker.jpg")
```

## What we do in Session 3+4: Part C

So far we learned  

- difference between correlation and causality
- to represent economic relations using graphs and regressions 
- omitting a variable may change sign of a relation 

Here we learn 

- the formal concept  of simple regression (OLS)
- that OLS is causal as long as *ceteris paribus* holds
- graphical intuition of the OLS estimator 
- how to interpret coefficients and interpret the "fit"

... and how to predict better prices of wine than experts!   

## Concept of Simple Regressions

Two variables $y$ and $x$ from a population (e.g., clients, firms)

- "explaining $y$ in terms of $x$," or 
- "studying how $y$ changes when we change $x$."

<br>

Some examples include:

- $y$ is the *demand* of ice cream and $p$ is the price
- $y$ is  *market value* of a footballer and $x$ the *goals* s/he scored
- $y$ is the *price of wine* and $x$ the *weather*
- $y$ is *hourly wage* and $x$ is *years of education*

## The Simple Regression Model

***Three issues*** if we want to “explain $y$ in terms of $x$”

- Which functional relationship do we assume (e.g., linear)? 
- How do we allow factors, other than $x$, to affect $y$? 
- How can we ensure a causal relationship between $y$ and $x$?  
  - *ceteris paribus* notion: holding constant other factors

## To address those 3 issues, we define

$$y=\beta_0+\beta_1x+u$$

- Linear relation between observables $x$ and $y$ (*Issue 1*)
- $\beta_0$: **intercept parameter** aka the **constant term**
- $\beta_1$: **slope parameter** in the relation between $y$ and $x$
- Error term $u$: Unobservables allowed to enter (*Issue 2*)
- Certain assumptions to get causal effect of $x$ on $y$ (*Issue 3*)  

Main intuition for causality: given we observe $x$ and $y$

- estimate $\beta_1$ and $\beta_0$ making assumptions on $u$
- $\beta_1$: how changes in $x$ affect changes in $y$ given $\beta_0$ and $u$

## We get the causal effect of $x$ on $y$ 

Ceteris paribus: we hold the unobserved factors in $u$ fixed

- so that the change in $u$ is zero, i.e., $\Delta{u}$= 0, 
- i.e., $u$ does not affect $y$
$$\Delta{y}=\beta_1\Delta{x}~~~~if~~~~\Delta{u}=0$$ 

- then $x$ has a linear effect on $y$ 
- where $\beta_1$ yields causal effect of $x$ on $y$ 



## Example: A Simple Wage Equation
  
Relating a person's wage to education and unobservables:  
          
$$wage=\beta_0+\beta_1 educ + u$$ 

- $wage$ is dollars earned per hour and $educ$ is education years, 
- $\beta_1$ measures the change in hourly wage given another year of education, holding all other factors fixed. 
- Some of the unobserved factors include 
  - labor force experience, 
  - innate ability, 
  - tenure with current employer, 
  - work ethic, 

$$\Delta{wage}=\beta_1\Delta educ$$



## Example: Bob's Ice Cream Business

Similarly, $\beta_1$ measures the causal effect of price on quantity

$$quantity=\beta_0+\beta_1 price + u$$

if we hold unobservable factors in $u$ fixed, 

- e.g., seasonality has no systematic impact on quantity

Only then we get the causal effect: 

$$\Delta{quantity}=\beta_1\Delta price$$

## Zero conditional mean assumption

How can we learn about the causal effect of $x$ on $y$, holding other factors fixed, if we ignore all those other factors?

We assume: $\mathbb{E}[u|x]=\mathbb{E}[u]=0$

Take the expected value of $y|x$.   

Then: $\mathbb{E}[y|x]= \mathbb{E}[\beta_0 +\beta_1x + u|x] = \beta_0 +\beta_1x + \mathbb{E}[u|x]$

which yields

$$\mathbb{E}(y|x)=\beta_0+\beta_1x$$

$\beta_1$ is causal due to this *zero conditional mean assumption*

## Harmless assumption $E(u)=0$

.. says nothing about the relation between $u$ and $x$, but simply makes a statement about the distribution of unobservables

We can always redefine the intercept in $y=\beta_0+\beta_1x+u$ to make $\mathbb{E}(u)=0$ true.

  - Soybean example: we can normalize the unobserved factors affecting soybean yield, such as land quality, to 0
  - Wage example: We can assume that things such as average ability are zero in the population of all working people.


## Less harmless: mean independence

or $\mathbb{E}(u|x)=u$ 

- for any $x$, we can obtain the expected (or average) value of $u$ for that slice of the population described by the value of $x$.
- The crucial assumption is that the average value of $u$ does not depend on the value of $x$.
- When assumption $E(u|x)=E(u)$ holds, we say that $u$ is **mean independent** of $x$. 
- When we combine mean independence with assumption $\mathbb{E}(u)=0$, we obtain the **zero conditional mean assumption**, $E(u|x)=0$.

## Wage example with $u=abil$

Equation $\mathbb{E}(u|x)=\mathbb{E}(u)$ requires that the average level of ability is the same, regardless of years of education $x$.
  
- For example, if $E(abil|8)$ denotes the average ability for the group of all people with eight years of education, 
- and $E(abil|16)$ denotes the average ability among people in the population with sixteen years of education, 
- $E(u|x)=E(u)$ implies that $E(abil|8)=E(abil|18)$

Thus, the average ability the same for all education levels.

## Is this a reasonable assumption?


If, for example, we think that average ability increases with years of education, then equation $E(u|x)=E(u)$ is false.

- This would happen if, on average, people with more ability choose to become more educated.
- As we cannot observe innate ability, we have no way of knowing whether or not average ability is the same for all education levels.
- But: this is an issue that we must address before relying on simple regression analysis!

## Bob's ice cream business 

For equation $\mathbb{E}(u|x)=\mathbb{E}(u)$ with

- $x=price$
- $u=seasonality$

it must hold $\mathbb{E}(summer|price)=\mathbb{E}(winter|price)$

- i.e., $u=seasonality$ is uncorrelated to $x=prices$ 
- remember higher demand in summer leads to higher prices
  - observing a high price, we expect a high demand shock
  - observing a low price, we expect a low demand shock



## The Ordinary Least Squares Estimator

How to estimate the parameters $\beta_0$ and $\beta_1$ in equation $y=\beta_0+\beta_1x+u$?

- To do this, we need a sample from the population.
- Let {($x_i,y_i):i=1,...,n$} denote a random sample of size $n$ from the
population.
- We can write $y_i=\beta_0+\beta_1x_i+u_i$ for each $i$.

As an example, $x_i$ might be the annual income and $y_i$ the annual savings for family $i$ during a particular year.

- If we have collected data on 15 families, then _n=15_.


## Savings and income for 15 families

aim is to get: $\mathbb{E}(savings|income)=\beta_0+\beta_1 income$


```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_2.png")
```

##  OLS Graph: Fitted values and residuals

Idea: choose $(\beta_0,\beta_1)$ to minimizes the squared sum of $u_i$

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wool_2_4.png")
```



## The Ordinary Least Squares Estimator

The OLS estimator has the following objective function:

$$(\hat{\beta}_{0}^{OLS}, \hat{\beta}_{1}^{OLS})=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i\hat{u}_i^2]}=\underset{(\hat{\beta}_{0}, \hat{\beta}_{1})}{argmin[\sum_i(\hat{y}_i-y_i)^2]}$$ 

Algorithm chooses $(\hat{\beta}_{0}, \hat{\beta}_{1})$ until $\hat{u}_i^2$ reaches global minimum

- $\hat{y}_i$ are the predicted values, 
- $\hat{\beta_0}$ and $\hat{\beta_1}$ are estimated coefficients, 
- $\hat{u}_i$ are residuals


## OLS: how to estimate $\beta_0$ and $\beta_1$?

The ordinary least squares (OLS) estimators are  

$$\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{Cov(x,y)}{Var(x)}$$

$$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$

Based on these estimated parameters, the OLS regression line is  

$$\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$$

Given observations $x_i$ and $y_i$, we just need to calculate 

- the mean of $\bar{y},\bar{x}$
- more commonly $Cov(x,y)$ and $Var(x)$. 

## SST, SSE, SSR
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("SST.jpg")
```


## R2 

How much of our model is explained by the regression line? 

$$\frac{SSE}{SST}$$

## root mean squared error

The residuals are the differences between the observed values (price) and the predicted values (calculated as predicted_price <- alpha + beta * df$AGST).

We square each residual, take the average (i.e., the mean of the squared residuals which gives the Mean Squared Error or MSE), and then take the square root of that average to get the RMSE.


## Hypothesis Testing

Hypothesis testing is a statistical method that allows us to make inferences or draw conclusions about a population based on a sample of data.

There are two types of hypotheses in a hypothesis test: Null hypothesis (H0) and Alternative hypothesis (Ha). 

- Null Hypothesis (H0): This is a statement about the population that either is believed to be true or is used to put forth an argument unless it can be shown to be incorrect beyond a reasonable doubt.

- Alternative Hypothesis (Ha): This is a claim about the population that is contradictory to the null hypothesis and what we conclude when we reject the null hypothesis.

In linear regression, the standard error can be used to construct confidence intervals for the coefficients, or to perform hypothesis tests.

## Key Statistics

- standard error 
- confidence intervals
- t-test
- p-value
- F-test

### Standard Error

The standard error is a measure of the variability in the mean from one sample to the next. It measures the accuracy with which a sample represents a population.
- It's a measure of the precision of your estimate. Smaller standard errors mean you have more precise (i.e., less variable) estimates.
- if we were to draw from a random sample and estimate our parameter of interest $N$, then the standard deviation of the estimated parameters are equal to the standard error 
- It represents the typical amount that our estimate varies across different random samples due to sampling error. Smaller standard errors mean our estimate is more precise and varies less across different samples.

 
Example: 

### Confidence Interval

The confidence interval provides a range of values, derived from the sample, that is likely to contain the true population value. It gives us an estimate of the uncertainty of our sample estimate.

- A 95% confidence interval means that if we were to draw a large number of random samples from the population and construct a 95% confidence interval from each sample, approximately 95% of these intervals would contain the true population parameter.

Example: 


### t-Test

The t-Test compares two averages (means) and tells us if they are different from each other. The t-Test also tells us how significant the differences are.

- In the context of regression, it is used to test whether a coefficient is significantly different from zero. The test statistic for each coefficient is the coefficient estimate divided by its standard error. This measures how many standard deviations the coefficient is from 0. If the coefficient is far from zero relative to the standard error, then we can reject the null hypothesis that the true coefficient value is zero.
 
- calculated as: 
- Rule of thumb: should be la

Example: 

### p-value

The p-value is a measure of the evidence against the null hypothesis provided by the data. 

The p-value is the probability that the results of your test occurred at random. If p-value is small (less than 0.05), we reject the null hypothesis.

 If we repeatedly drew a large number of random samples from a population where the null hypothesis is true, the p-value represents the proportion of those samples where we would observe a t-statistic as extreme or more extreme than the one calculated from our sample. A small p-value (typically less than 0.05) indicates that such an extreme t-statistic would be unlikely if the null hypothesis were true, so we reject the null hypothesis.
 


##

F-Test: The F-test in regression compares the model with no predictors (just the intercept) to your model. The null hypothesis is that all coefficients (except the intercept) are zero, and the alternative hypothesis is that at least one is not zero. The test statistic is a ratio of mean squared error (MSE) from regression and the MSE of residuals. If the p-value for the F-test is small (typically ≤ 0.05), we reject the null hypothesis that all slope coefficients are zero, suggesting that at least one predictor is useful in the model.

# Simple OLS regression applied to the case of Bordeaux wine prices

## Background on Bordeaux wine prices 

There are differences in price and quality of wine over time: 

- wines are tasting better when they are older. 
- incentive to store young wines until they are mature. 
- Wine experts taste the wine and then predict future prices based on taste 

Can you really predict prices of mature wine based on taste of young wine?

- the taste will change significantly over time 

Ashenfelter challenged "experts" using his Econometrics skills 

## Maverick Ashenfelter made it into NYT

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_nyt.jpg")
```

- Calculate the winter rain and the harvest rain (in millimeters). 
- Add summer heat in the vineyard (in degrees centigrade). 
- What do you have? 
- A very, very passionate argument over wine.


## Ashenfelter made it into Forbes 

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_forbes.jpg")
```

<br>

... although few hundred observations is not really big data 

## 
<iframe width="560" height="315" src="https://www.youtube.com/embed/8WMRj9mTQtI" frameborder="0" allowfullscreen></iframe>

##  Look at this table from the paper 

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("wine_table2.jpg")
```


## Answer the following question

What predicts the prices of older, mature wines?

- Dry Summers, 
- Humid Summers,
- Cold Summers

## Answer the following question

What is the relationship between wine price and rain in August?

- Positively correlated + statistically insignificant 
- Positively correlated + statistically significant 
- Negatively correlated + statistically insignificant
- Negatively correlated + statistically significant

## Answer the following question
Which independent variable has the largest effect on the price of Bordeaux wine? Just evaluate the magnitude of the effect

- Age of the vintage
- Average temperature in growing season
- Rain in August
- Average temperature in September

<br>
(we look later at t-values and p-values for statistical significance)

## Answer the following question
With every additional year, the price of a vintage increases by (column 2)

- 0.024% 
- 0.048%
- 2.4%
- 4.8%

## Answer the following question
According to the table above, the famous Ashenfelter Wine Equation can be represented as:                                      

$$
\begin{aligned}
+ & 0.024~~~~Age \\
+ & 0.620~~~~Avg. Temp. ~(growing~season) \\
– & 0.004~~~~August~rain \\
+ & 0.001~~~~Pre-vintage~rain \\ 
+ & 0.008~~~~Temp. (September)
\end{aligned}
$$

## Answer the following question

It is a good example for regressions because wine prices have an effect on weather but not weather on wine prices

- Yes 
- No 
- Maybe


## Answer the following question

Concluding from the question above, we do not have to care about unobserved confounding factors

- Yes 
- No
- Maybe

## Solutions

1. Dry Summers
2. Negatively correlated + statistically significant
3. Average temperature in growing season
4. 2.4\%
5. No
6. No
7. No

## 
to do: log prices
R2
Root mean squared error
Standard errors 

not reported: p-values, t-test, F-test

## Let's have a look how the magic wine equation works

1. Load the data set ('wine.csv').
2. Regress average temperature in the growing season on prices. What do you see? How do you interpret the slope?
3. Calculate OLS by hand using the formula from class
4. Add the other control variables
5. How do you measure the "fit" of a model? Which of the two models has the better fit? 

## 
Regression of Average Temperature in Growing Season on Price 
```{r, echo=TRUE}
library("rio")
df <- import("wine.csv")

lm(Price ~ AGST, data=df) 
```
## 
```{r, echo=FALSE}
y <-df$Price
x <-df$AGST

# Add regression line
plot(x, y, main = "Regression of AGST on Price",
     xlab = "AGST (Average Temperature in Growing Season)", ylab = "Prices",
     pch = 19, frame = FALSE)
abline(lm(y ~ x, data = df), col = "blue")

```

## 

```{r}
fit <- lm(Price ~ AGST, ,data=df) 
summary(fit)

```

## OLS by hand
```{r, echo=TRUE}
# Load necessary library
library(dplyr)

# Assume df is your dataframe and it has columns price and AGST

# Calculate number of observations
n <- nrow(df)

# Calculate means
mean_price <- mean(df$Price)
mean_AGST <- mean(df$AGST)

# Calculate the parts of the formula for Beta
beta_numerator <- sum((df$Price - mean_price) * (df$AGST - mean_AGST))
beta_denominator <- sum((df$AGST - mean_AGST)^2)

# Calculate Beta (slope)
beta <- beta_numerator / beta_denominator

# Calculate Alpha (intercept)
alpha <- mean_price - beta * mean_AGST

# Print the coefficients
cat("Alpha: ", alpha, "\n")
cat("Beta: ", beta, "\n")

```
## 

- SSE: This is the sum of the squares of the residuals. It measures the unexplained variability by the model.
- SST: This is the total sum of squares, which measures the total variability in the dependent variable, price.
- SSR: This is the sum of squares due to regression, which measures the variability explained by the regression model. It is calculated as the difference between SST and SSE.

In a simple linear regression model, the sums of squares satisfy the equation: SST = SSR + SSE.

##  
```{r}
# Using the previously calculated alpha and beta

# Predicted values
predicted_price <- alpha + beta * df$AGST

# Residuals
residuals <- df$Price - predicted_price

# Sum of Squares Error (SSE)
SSE <- sum(residuals^2)

# Total Sum of Squares (SST)
SST <- sum((df$Price - mean_price)^2)

# Sum of Squares Regression (SSR)
SSR <- SST - SSE

# Print the sums of squares
cat("SSE: ", SSE, "\n")
cat("SST: ", SST, "\n")
cat("SSR: ", SSR, "\n")
```
## RMSE

The residuals are the differences between the observed values (price) and the predicted values (calculated as predicted_price <- alpha + beta * AGST).

We square each residual, take the average (i.e., the mean of the squared residuals which gives the Mean Squared Error or MSE), and then take the square root of that average to get the RMSE.

RMSE is a useful measure because it's in the same units as the dependent variable and lower values indicate better fit to the data.

## 
```{r}
# Calculate the Root Mean Squared Error (RMSE)
RMSE <- sqrt(mean(residuals^2))

# Print the RMSE
cat("RMSE: ", RMSE, "\n")
```

## 
The R-squared value is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by the independent variable(s) in a regression model.

R-squared values range from 0 to 1. A value of 0 indicates that the dependent variable cannot be predicted from the independent variable. A value of 1 indicates the dependent variable can be predicted without error from the independent variable. An R-squared between 0 and 1 indicates the extent to which the dependent variable is predictable. An R-squared of 0.20 means that 20 percent of the dependent variable, price, can be explained by the independent variable, AGST.

## 
```{r, echo=TRUE}
# Calculate R-squared
R_squared <- SSR / SST

# Print the R-squared
cat("R-squared: ", R_squared, "\n")
```



## 
OLS by construction minimizes the distance from the observation point to the regression line, which allows us to exploit some information from it:

- SSE: the distance from observation point to regression line is the error that our model makes. The part that is not explained by the model
- SSR: the sum of the differences between the predicted value and the mean of the dependent variable SSR=(1-SSE)
a measure that describes how well our line fits the data
- SST: is the difference between the observed dependent variable and its mean a measure of total variation

## How do you measure the "fitness" of a model?

Every model makes errors.
These residuals are sometimes used to measure how good is a model and to compare models against each other.
Goodness if fit:

measured by explained part of our model in relation
R2: the fraction of explained variation and total variation (R2=SSR/SST)


## 
For now, we only regress prices on average temperature in the growing season

What else can we include into the model?

```{r}
head(df)
```

## 
```{r}
fit <- lm(Price ~ AGST + WinterRain + HarvestRain + Age, data=df) 
summary(fit)

```

## Robert Parker, the world’s most influential wine expert said:


```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("RobParker.jpg")
```


## Ashenfelter responds

```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("Ashenfelter.jpg")
```

## So who is right?

Years ago, many tasters criticized Prof. Ashenfelter when he announced that his data indicated the 1986 vintage is average, 

- despite the belief of many tasters the wine would age incredibly well. 

History has supported his claims. 

- Today, many people consider the 1986 vintages to be average wines.


## There is even an app now
```{r , echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("shinyapp.jpg")
```

## What have we learned?

- 
